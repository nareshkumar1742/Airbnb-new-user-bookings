{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJPZ3qPsUR_D"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from joblib import load\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from xgboost import XGBClassifier\n",
        "import timeit\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "##################################################################\n",
        "def age_to_bucket(val):\n",
        "  \"\"\"\n",
        "  Here we are mapping bucket value to the corresponding age value\n",
        "  When a value enters the function the age_bucket_dict is looped individually.\n",
        "  If the value is in the particular array then its corresponding key will be returned.\n",
        "  Here each key value consist of 5 consecutive age while the last one consist og 6 consecutive age.\n",
        "\n",
        "  parameter:\n",
        "    val    : int\n",
        "\n",
        "  returns:\n",
        "    key    : string\n",
        "  \"\"\"\n",
        "  age_bucket_dict={'bucket1':np.arange(15,20),'bucket2':np.arange(20,25),\n",
        "                   'bucket3':np.arange(25,30),'bucket4':np.arange(30,35),\n",
        "                   'bucket5':np.arange(35,40),'bucket6':np.arange(40,45),\n",
        "                   'bucket7':np.arange(45,50),'bucket8':np.arange(50,55),\n",
        "                   'bucket9':np.arange(55,60),'bucket10':np.arange(60,65),\n",
        "                   'bucket11':np.arange(65,70),'bucket12':np.arange(70,75),\n",
        "                   'bucket13':np.arange(75,80),'bucket14':np.arange(80,85),\n",
        "                   'bucket15':np.arange(85,90),'bucket16':np.arange(90,95),\n",
        "                   'bucket17':np.arange(95,100),'bucket18':np.arange(100,105),\n",
        "                   'bucket19':np.arange(105,110),'bucket20':np.arange(110,116),\n",
        "                   'nullbucket': [-1]\n",
        "                   }\n",
        "  for key,value in age_bucket_dict.items():\n",
        "    if val in value:\n",
        "      return key\n",
        "\n",
        "##################################################################\n",
        "def sessions_col_names():\n",
        "  \"\"\"\n",
        "  Returns the column names of sessions data.\n",
        "  \n",
        "  parameter:\n",
        "  index_of_action, index_of_action_details, index_of_action_type, index_of_device_type\n",
        "\n",
        "  returns:\n",
        "  column_names: list (column names of sessions dataset) \n",
        "\n",
        "  \"\"\"\n",
        "  index_of_action=pd.read_csv('index_of_action.csv',index_col=0).T\n",
        "  index_of_action_details=pd.read_csv('index_of_action_details.csv',index_col=0).T\n",
        "  index_of_action_type=pd.read_csv('index_of_action_type.csv',index_col=0).T\n",
        "  index_of_device_type=pd.read_csv('index_of_device_type.csv',index_col=0).T\n",
        "\n",
        "  \n",
        "  column_names=[]\n",
        "\n",
        "  #count\n",
        "  column_names.append('id')\n",
        "  column_names.append('no_of_records')\n",
        "  to_add_list=[0]*len(index_of_action.columns)\n",
        "  for i in index_of_action.keys():\n",
        "    to_add_list[index_of_action[i][0]]=i+' (count)'\n",
        "  column_names=column_names+to_add_list\n",
        "  to_add_list=[0]*len(index_of_action_details.columns)\n",
        "  for i in index_of_action_details.keys():\n",
        "    to_add_list[index_of_action_details[i][0]]=i+' (count)'\n",
        "  column_names=column_names+to_add_list\n",
        "  to_add_list=[0]*len(index_of_action_type.columns)\n",
        "  for i in index_of_action_type.keys():\n",
        "    to_add_list[index_of_action_type[i][0]]=i+' (count)'\n",
        "  column_names=column_names+to_add_list\n",
        "  to_add_list=[0]*len(index_of_device_type.columns)\n",
        "  for i in index_of_device_type.keys():\n",
        "    to_add_list[index_of_device_type[i][0]]=i+' (count)'\n",
        "  column_names=column_names+to_add_list\n",
        "  column_names.append('sum_of_secs_elapsed')\n",
        "  column_names.append('mean_of_secs_elapsed')\n",
        "  column_names.append('std_of_secs_elapsed')\n",
        "  column_names.append('median_of_secs_elapsed')\n",
        "  column_names.append('less_than_equal_60sec_count')\n",
        "  column_names.append('1min_to_10min_count')\n",
        "  column_names.append('10min_to_1hr_count')\n",
        "  column_names.append('1hr_to_1day_count')\n",
        "  column_names.append('more_than_1day_count')\n",
        "  to_add_list=[0]*len(index_of_action.columns)\n",
        "  for i in index_of_action.keys():\n",
        "    to_add_list[index_of_action[i][0]]=i+' (action presence)%'\n",
        "  column_names=column_names+to_add_list\n",
        "  to_add_list=[0]*len(index_of_action_type.columns)\n",
        "  for i in index_of_action_type.keys():\n",
        "    to_add_list[index_of_action_type[i][0]]=i+' (action type presence)%'\n",
        "  column_names=column_names+to_add_list\n",
        "\n",
        "  return column_names\n",
        "\n",
        "##################################################################\n",
        "def dcg_score(y_true, y_score, k=5):\n",
        "    \"\"\"\n",
        "    Discounted Cummulative Gain(dcg)= sum(i:1 to k) ((2**relavance(i))-1)/(log2(i+1))\n",
        "    Here y_true is array which is label encoded and y_score is the probability score\n",
        "    eg:\n",
        "      y_true=[0,0,0,1,0,0,0,0,0,0,0,0] (For destination ES)\n",
        "      y_score=[0,0,0,0.70,0,0.025,0,0.15,0,0,0.1,0.025]\n",
        "    we argsort in highest order of y_score\n",
        "    order=[3,6,9,5,12,......]\n",
        "    now we take the top 5 index of the order and pick values of those top 5 index from y_true.\n",
        "    gain=[(2**1)-1,(2**0)-1,(2**0)-1,...]\n",
        "    discount=[log2(1+1),log2(2+1),log2(3+1),...]\n",
        "    dcg=sum(gain/discount)\n",
        "\n",
        "    parameter:\n",
        "      y_true : numpy array of shape [n_samples,number_of_classes]\n",
        "      y_score: numpy array of shape [n_samples,number_of_classes]\n",
        "      k      : int (default=5)\n",
        "    \n",
        "    returns:\n",
        "      dcg    : float\n",
        "    \"\"\"\n",
        "\n",
        "    order = np.argsort(y_score)[::-1]\n",
        "    y_true = np.take(y_true, order[:k])\n",
        "\n",
        "    gain = 2 ** y_true - 1\n",
        "\n",
        "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
        "    return np.sum(gain / discounts)\n",
        "  \n",
        "def ndcg_score(ground_truth, predictions, k=5):\n",
        "    \"\"\"\n",
        "    Normalized Discounted Cummulative Gain (ndcg)= Discounted Cummulative Gain/Ideal Discounted Cummulative Gain\n",
        "    idcg=sum(i: 1 to k)((2**true_relavance(i))-1)/(log2(i+1))\n",
        "    In idcg the probability score (ie y_score) will be y_true\n",
        "\n",
        "    parameter:\n",
        "      ground_truth: numpy array of shape [n_samples,]\n",
        "      predictions : numpy array of shape [n_samples,number_of_classes]\n",
        "      k           : int (default=5)\n",
        "\n",
        "    returns:\n",
        "      score       : float\n",
        "      (average ndcg score) \n",
        "    \"\"\"\n",
        "    \n",
        "    lb = LabelBinarizer()\n",
        "    lb.fit(range(predictions.shape[1] + 1))\n",
        "    T = lb.transform(ground_truth)\n",
        "\n",
        "    scores = []\n",
        "\n",
        "    # Iterate over each y_true and compute the DCG score\n",
        "    for y_true, y_score in zip(T, predictions):\n",
        "        actual = dcg_score(y_true, y_score, k)\n",
        "        best = dcg_score(y_true, y_true, k)\n",
        "        if best == 0:\n",
        "            best = 0.000000001\n",
        "        score = float(actual) / float(best)\n",
        "        scores.append(score)\n",
        "\n",
        "    return np.mean(scores)\n",
        "\n",
        "##################################################################\n",
        "def preprocess_users(user_data):\n",
        "  \"\"\"Function that is defined to preprocess the users data\"\"\"\n",
        "\n",
        "  user_data=user_data.reset_index()\n",
        "\n",
        "  selected_language=np.load('selected_language.npz.npy',allow_pickle=True)\n",
        "  selected_browsers=np.load('selected_browsers.npz.npy',allow_pickle=True)\n",
        "  selected_aff_provider=np.load('selected_aff_provider.npz.npy',allow_pickle=True)\n",
        "  selected_aff_track=np.load('selected_aff_track.npz.npy',allow_pickle=True)\n",
        "\n",
        "  user_data['date_account_created']=pd.to_datetime(user_data['date_account_created'])\n",
        "  user_data['day_account_created']=pd.DatetimeIndex(user_data['date_account_created']).dayofweek\n",
        "  user_data['year_account_created']=pd.DatetimeIndex(user_data['date_account_created']).year\n",
        "  user_data['timestamp_first_active']=pd.to_datetime(user_data['timestamp_first_active'],format='%Y%m%d%H%M%S')\n",
        "  user_data['day_first_active']=pd.DatetimeIndex(user_data['timestamp_first_active']).dayofweek\n",
        "  user_data['hour_first_active']=pd.DatetimeIndex(user_data['timestamp_first_active']).hour\n",
        "\n",
        "  for i in range(len(user_data['age'])):\n",
        "    if user_data['age'][i]>150 and user_data['age'][i]<2010:\n",
        "      user_data['age'][i]=user_data['year_account_created'][i]-user_data['age'][i]\n",
        "  user_data['age']=user_data.age.apply(lambda x: 115 if x>115 else x)\n",
        "  user_data['age']=user_data.age.apply(lambda x: 15 if x<15 else x)\n",
        "  user_data['age'].fillna(-1,inplace=True)\n",
        "  user_data['first_affiliate_tracked'].fillna('unknown',inplace=True)\n",
        "  user_data['age_bucket']=user_data.age.apply(age_to_bucket)\n",
        "  user_data['language']=user_data.language.apply(lambda x: 'Other' if x not in selected_language else x)\n",
        "  user_data['first_browser']=user_data.first_browser.apply(lambda x: 'Other' if x not in selected_browsers else x)\n",
        "  user_data['affiliate_provider']=user_data.affiliate_provider.apply(lambda x: 'smallcontribution' if x not in selected_aff_provider else x)\n",
        "  user_data['first_affiliate_tracked']=user_data.first_affiliate_tracked.apply(lambda x: 'others' if x not in selected_aff_track else x)\n",
        "  user_data['no_of_nans']=0\n",
        "  for i in range(len(user_data)):\n",
        "    if user_data['age'][i]==-1:\n",
        "      user_data['no_of_nans'][i]+=1\n",
        "    if user_data['gender'][i]=='-unknown-':\n",
        "      user_data['no_of_nans'][i]+=1\n",
        "    if user_data['first_affiliate_tracked'][i]=='untracked':\n",
        "      user_data['no_of_nans'][i]+=1\n",
        "\n",
        "  user_data.drop(['date_account_created','timestamp_first_active',\n",
        "                  'year_account_created','date_first_booking'],axis=1,inplace=True)\n",
        "\n",
        "  age_bucket_ohe=load('age_bucket_ohe.bin')\n",
        "  user_age_bucket_ohe=pd.DataFrame(age_bucket_ohe.transform(user_data['age_bucket'].values.reshape(-1,1)).todense())\n",
        "\n",
        "  gender_ohe=load('gender_ohe.bin')\n",
        "  user_gender_ohe=pd.DataFrame(gender_ohe.transform(user_data['gender'].values.reshape(-1,1)).todense())\n",
        "\n",
        "  language_ohe=load('language_ohe.bin')\n",
        "  user_language_ohe=pd.DataFrame(language_ohe.transform(user_data['language'].values.reshape(-1,1)).todense())\n",
        "\n",
        "  signup_method_ohe=load('signup_method_ohe.bin')\n",
        "  user_signup_method_ohe=pd.DataFrame(signup_method_ohe.transform(user_data['signup_method'].values.reshape(-1,1)).todense())\n",
        "  \n",
        "  affiliate_channel_ohe=load('affiliate_channel_ohe.bin')\n",
        "  user_affiliate_channel_ohe=pd.DataFrame(affiliate_channel_ohe.transform(user_data['affiliate_channel'].values.reshape(-1,1)).todense())\n",
        "\n",
        "  affiliate_provider_ohe=load('affiliate_provider_ohe.bin')\n",
        "  user_affiliate_provider_ohe=pd.DataFrame(affiliate_provider_ohe.transform(user_data['affiliate_provider'].values.reshape(-1,1)).todense())\n",
        "  \n",
        "  first_affiliate_tracked_ohe=load('first_affliate_tracked_ohe.bin')\n",
        "  user_first_affiliate_tracked_ohe=pd.DataFrame(first_affiliate_tracked_ohe.transform(user_data['first_affiliate_tracked'].values.reshape(-1,1)).todense())\n",
        "\n",
        "  signup_app_ohe=load('signup_app_ohe.bin')\n",
        "  user_signup_app_ohe=pd.DataFrame(signup_app_ohe.transform(user_data['signup_app'].values.reshape(-1,1)).todense())\n",
        "\n",
        "  first_device_type_ohe=load('first_device_type_ohe.bin')\n",
        "  user_first_device_type_ohe=pd.DataFrame(first_device_type_ohe.transform(user_data['first_device_type'].values.reshape(-1,1)).todense())\n",
        "  \n",
        "  first_browser_ohe=load('first_browser_ohe.bin')\n",
        "  user_first_browser_ohe=pd.DataFrame(first_browser_ohe.transform(user_data['first_browser'].values.reshape(-1,1)).todense())\n",
        "\n",
        "  preprocessed_user=pd.concat([user_age_bucket_ohe,user_gender_ohe,user_language_ohe,user_signup_method_ohe,\n",
        "                               user_affiliate_channel_ohe,user_affiliate_provider_ohe,user_first_affiliate_tracked_ohe,\n",
        "                               user_signup_app_ohe,user_first_device_type_ohe,user_first_browser_ohe],axis=1)\n",
        "  \n",
        "  user_data.reset_index(inplace=True)\n",
        "  user_data.drop('index',axis=1,inplace=True)\n",
        "\n",
        "  if 'country_destination' in user_data.columns:\n",
        "    preprocessed_user=pd.concat([user_data[['id','country_destination','day_account_created',\n",
        "                                            'day_first_active','hour_first_active',\n",
        "                                            'signup_flow','age','no_of_nans']],preprocessed_user],axis=1,join='inner')\n",
        "  else:\n",
        "    preprocessed_user=pd.concat([user_data[['id','day_account_created',\n",
        "                                            'day_first_active','hour_first_active',\n",
        "                                            'signup_flow','age','no_of_nans']],preprocessed_user],axis=1,join='inner')\n",
        "  \n",
        "  return preprocessed_user\n",
        "\n",
        "#############################################################\n",
        "def preprocess_sessions(sessions_data):\n",
        "  \"\"\"Function that preprocess sessions dataset\"\"\"\n",
        "\n",
        "  sessions_data=sessions_data.reset_index()\n",
        "\n",
        "  index_of_action=pd.read_csv('index_of_action.csv',index_col=0).T\n",
        "  index_of_action_details=pd.read_csv('index_of_action_details.csv',index_col=0).T\n",
        "  index_of_action_type=pd.read_csv('index_of_action_type.csv',index_col=0).T\n",
        "  index_of_device_type=pd.read_csv('index_of_device_type.csv',index_col=0).T\n",
        "\n",
        "  sessions_data.action.fillna(\"unkown_action\",inplace=True)\n",
        "  sessions_data.action_detail.fillna('unknow_action_detail',inplace=True)\n",
        "  sessions_data.action_type.fillna('unkown_action_type',inplace=True)\n",
        "  sessions_data.device_type.fillna('unkown_device_type',inplace=True)\n",
        "\n",
        "  is_in_train={}\n",
        "  for i in sessions_data.action.unique():\n",
        "    is_in_train[i]=0\n",
        "  for i in index_of_action.columns:\n",
        "    is_in_train[i]=1\n",
        "  sessions_data.action=sessions_data.action.apply(lambda x: 'OTHER' if is_in_train[x]==0 else x)\n",
        "  is_in_train={}\n",
        "  for i in sessions_data.action_type.unique():\n",
        "    is_in_train[i]=0\n",
        "  for i in index_of_action_type.columns:\n",
        "    is_in_train[i]=1\n",
        "  sessions_data.action_type=sessions_data.action_type.apply(lambda x: 'unkown_action_type' if is_in_train[x]==0 else x)\n",
        "  is_in_train={}\n",
        "  for i in sessions_data.action_detail.unique():\n",
        "    is_in_train[i]=0\n",
        "  for i in index_of_action_details.columns:\n",
        "    is_in_train[i]=1\n",
        "  sessions_data.action_detail=sessions_data.action_detail.apply(lambda x: 'unknow_action_detail' if is_in_train[x]==0 else x)\n",
        "  is_in_train={}\n",
        "  for i in sessions_data.device_type.unique():\n",
        "    is_in_train[i]=0\n",
        "  for i in index_of_device_type.columns:\n",
        "    is_in_train[i]=1\n",
        "  sessions_data.device_type=sessions_data.device_type.apply(lambda x: 'unkown_device_type' if is_in_train[x]==0 else x)\n",
        "  \n",
        "  sess_id_group=sessions_data.groupby(['user_id'])\n",
        "  user_samples = []\n",
        "  ln = len(sess_id_group)\n",
        "  for id_session in sess_id_group:\n",
        "    details_of_id = id_session[1]\n",
        "    l = []\n",
        "    \n",
        "    #the id\n",
        "    l.append(id_session[0])\n",
        "    \n",
        "    #The actual first feature is the number of values.\n",
        "    l.append(len(details_of_id))\n",
        "\n",
        "    #For Action \n",
        "    #Number of times unique action value occurs\n",
        "    feature_action=[0]*len(index_of_action.columns)\n",
        "    for i,action in enumerate(details_of_id.action.values):\n",
        "      feature_action[index_of_action[action][0]]+=1\n",
        "    l=l+feature_action\n",
        "\n",
        "    #For action_detail\n",
        "    #Number of times unique action_detail value occurs\n",
        "    feature_action_detail=[0]*len(index_of_action_details.columns)\n",
        "    for i,action in enumerate(details_of_id.action_detail.values):\n",
        "      feature_action_detail[index_of_action_details[action][0]]+=1\n",
        "    l=l+feature_action_detail\n",
        "\n",
        "    #For action_type \n",
        "    #Number of times unique action_type value occurs\n",
        "    feature_action_type=[0]*len(index_of_action_type.columns)\n",
        "    for i,action in enumerate(details_of_id.action_type.values):\n",
        "      feature_action_type[index_of_action_type[action][0]]+=1\n",
        "    l=l+feature_action_type\n",
        "\n",
        "    #For Device type\n",
        "    #Number of times unique device_type value occurs\n",
        "    feature_device_type=[0]*len(index_of_device_type.columns)\n",
        "    for i,device in enumerate(details_of_id.device_type.values):\n",
        "      feature_device_type[index_of_device_type[device][0]]+=1\n",
        "    l=l+feature_device_type\n",
        "\n",
        "    #For seconds elapsed \n",
        "    secs=details_of_id.secs_elapsed.fillna(0).values\n",
        "    feature_secs=[0]*4\n",
        "    if len(secs)>0:\n",
        "      feature_secs[0]=np.sum(secs)\n",
        "      feature_secs[1]=np.mean(secs)\n",
        "      feature_secs[2]=np.std(secs)\n",
        "      feature_secs[3]=np.median(secs)\n",
        "    l=l+feature_secs\n",
        "\n",
        "    #Secs elapsed representation\n",
        "    feature_action_secs_elapsed_represent=[0]*5\n",
        "    details_of_id.secs_elapsed.fillna(0,inplace=True)\n",
        "    if len(secs)>0:\n",
        "      for i,action in enumerate(details_of_id.action.unique()):\n",
        "        sec_elapsed_sum=details_of_id[details_of_id.action==action]['secs_elapsed'].values.sum()\n",
        "        if sec_elapsed_sum<=60:#less than a minute\n",
        "          feature_action_secs_elapsed_represent[0]+=1\n",
        "        elif sec_elapsed_sum>60 and sec_elapsed_sum<=600:#between 1 minute to 10 minute\n",
        "          feature_action_secs_elapsed_represent[1]+=1\n",
        "        elif sec_elapsed_sum>600 and sec_elapsed_sum<=7200:#between 10 minutes to 2 hours\n",
        "          feature_action_secs_elapsed_represent[2]+=1\n",
        "        elif sec_elapsed_sum>7200 and sec_elapsed_sum<=86400:#between 2 hours to 24 hours\n",
        "          feature_action_secs_elapsed_represent[3]+=1\n",
        "        elif sec_elapsed_sum>86400: #greater than 24 hours\n",
        "          feature_action_secs_elapsed_represent[4]+=1        \n",
        "    l=l+feature_action_secs_elapsed_represent\n",
        "\n",
        "    #Percentage of presence of an action in the id\n",
        "    feature_action_presence=[0]*len(index_of_action.columns)\n",
        "    for i,action in enumerate(details_of_id.action.unique()):\n",
        "      feature_action_presence[index_of_action[action][0]]+=details_of_id[details_of_id.action==action].count()['action']/len(details_of_id.action)\n",
        "    l=l+feature_action_presence\n",
        "\n",
        "    #percentage of presence of action type in id\n",
        "    action_type_unique_counts=np.unique(details_of_id.action_type,return_counts=True)\n",
        "    unknown_action_index=np.where(action_type_unique_counts[0]=='unkown_action_type')\n",
        "    unique_action_type=np.delete(action_type_unique_counts[0],unknown_action_index)\n",
        "    unique_action_type_count=np.delete(action_type_unique_counts[1],unknown_action_index)\n",
        "    feature_action_type_presence=[0]*len(index_of_action_type.columns)\n",
        "    for i,action in enumerate(details_of_id.action_type.unique()):\n",
        "      if action!='unkown_action_type':\n",
        "        action_type_index=np.where(unique_action_type==action)\n",
        "        action_type_count=unique_action_type_count[action_type_index]\n",
        "        feature_action_type_presence[index_of_action_type[action][0]]+=action_type_count[0]/np.sum(unique_action_type_count)\n",
        "    l=l+feature_action_type_presence\n",
        "    user_samples.append(l)\n",
        "  \n",
        "  user_samples=np.array(user_samples)\n",
        "\n",
        "  column_names=sessions_col_names()\n",
        "\n",
        "  sessions_user_preprocessed=pd.DataFrame(user_samples,columns=column_names)\n",
        "  sessions_user_preprocessed.drop(['unkown_action_type (action type presence)%'],axis=1,inplace=True)  \n",
        "\n",
        "  return sessions_user_preprocessed\n",
        "\n",
        "####################################################################\n",
        "def final(user_data,sessions_data=[]):\n",
        "  \"\"\"\n",
        "  Takes in raw data that is user and session data and gives the top 5 preferred destination\n",
        "\n",
        "  parameter:\n",
        "    data   : pandas dataframe\n",
        "  \n",
        "  return: \n",
        "    id,top 5 prefered destination\n",
        "  \"\"\"\n",
        "  sessions_data_present=False\n",
        "  if len(sessions_data)>=1:\n",
        "    sessions_data_present=True\n",
        "\n",
        "  preprocessed_user=preprocess_users(user_data)#preprocessing users data\n",
        "\n",
        "  #if the sessions dataset is given the following code executes\n",
        "  if sessions_data_present:\n",
        "    user_counts=sessions_data['user_id'].value_counts()#Takes count of number of session record each user have\n",
        "    present_ids=[]#Take ids of those whose session record is given\n",
        "    null_ids=[]#Takes ids of those whose doesn't have sessions records\n",
        "    for i in range(len(user_counts)):\n",
        "        present_ids.append(user_counts.keys()[i])\n",
        "    left_out_ids=list(set(user_data['id'])-set(sessions_data['user_id']))#Ids that are in users data but not even one record in sessions data\n",
        "    null_ids.extend(left_out_ids)\n",
        "    \n",
        "    #present_sessions variable preprocesses sessions dataset for those ids whose session record is given\n",
        "    if len(present_ids)>=1:\n",
        "      present_sessions=sessions_data.loc[sessions_data['user_id'].isin(present_ids)]    \n",
        "      preprocessed_present_sessions=preprocess_sessions(present_sessions)\n",
        "      preprocessed_sessions=preprocessed_present_sessions\n",
        "    \n",
        "    #For those ids whose sessions dataset is not given are given zero values for all the features that are extracted from sessions dataset\n",
        "    if len(null_ids)>=1:\n",
        "      preprocessed_null_sessions=[]\n",
        "      for i in range(len(null_ids)):\n",
        "          null_session=[0]*667\n",
        "          null_session=[null_ids[i]]+null_session\n",
        "          preprocessed_null_sessions.append(null_session)\n",
        "      preprocessed_null_sessions=np.array(preprocessed_null_sessions)\n",
        "      column_names=sessions_col_names()\n",
        "      preprocessed_null_sessions=pd.DataFrame(preprocessed_null_sessions,columns=column_names)\n",
        "      preprocessed_null_sessions.drop(['unkown_action_type (action type presence)%'],axis=1,inplace=True) \n",
        "      #we are checking if there are some users with session dataset\n",
        "      #If so we have to concatenate with that of those whose session is not given\n",
        "      if len(present_ids)>=1:\n",
        "        preprocessed_sessions=pd.concat([preprocessed_sessions,preprocessed_null_sessions],axis=0)\n",
        "      else:\n",
        "        preprocessed_sessions=preprocessed_null_sessions\n",
        "\n",
        "  #If no sessions data is given the following else code snippet executes\n",
        "  else:\n",
        "    null_ids=user_data['id'].tolist()\n",
        "    preprocessed_null_sessions=[]\n",
        "    for i in range(len(null_ids)):\n",
        "        null_session=[0]*667\n",
        "        null_session=[null_ids[i]]+null_session\n",
        "        preprocessed_null_sessions.append(null_session)\n",
        "    preprocessed_null_sessions=np.array(preprocessed_null_sessions)\n",
        "    column_names=sessions_col_names()\n",
        "    preprocessed_null_sessions=pd.DataFrame(preprocessed_null_sessions,columns=column_names)\n",
        "    preprocessed_null_sessions.drop(['unkown_action_type (action type presence)%'],axis=1,inplace=True) \n",
        "    preprocessed_sessions=preprocessed_null_sessions\n",
        "\n",
        "  final_df=preprocessed_sessions.merge(preprocessed_user,how='inner',left_on='id',right_on='id')\n",
        "  final_df.drop(['id'],axis=1,inplace=True)\n",
        "  xgboost=XGBClassifier()\n",
        "  xgboost.load_model('xgboost_model.json')\n",
        "  le=load('class_label_encoder.bin')\n",
        "  proba_prediction=xgboost.predict_proba(np.array(final_df)).argsort(axis=1)[:,::-1] \n",
        "  prediction=le.classes_[proba_prediction[:,:5]]\n",
        "  return preprocessed_sessions['id'],prediction\n",
        "\n",
        "#########################################################################\n",
        "def final_2(user_data,target_value,sessions_data=[]):\n",
        "  \"\"\"\n",
        "  Takes in raw data that is user and session data and gives the ndcg score\n",
        "\n",
        "  parameter:\n",
        "    data        : pandas dataframe\n",
        "    target value: pandas dataframe (having id and country_destination)\n",
        "  \n",
        "  return: \n",
        "    ndcg score\n",
        "  \"\"\"\n",
        "  sessions_data_present=False\n",
        "  if len(sessions_data)>=1:\n",
        "    sessions_data_present=True\n",
        "\n",
        "  preprocessed_user=preprocess_users(user_data)#preprocessing users data\n",
        "\n",
        "  user_to_target=[]\n",
        "  #Creating a dataframe that has id and its corresponding destination\n",
        "  for target_index in range(len(target_value)):\n",
        "    user_to_target.append([user_data['id'].iloc[target_index],target_value[target_index]])\n",
        "  user_to_target=pd.DataFrame(user_to_target,columns=['id','target'])\n",
        "\n",
        "  #if the sessions dataset is given the following code executes\n",
        "  if sessions_data_present:\n",
        "    user_counts=sessions_data['user_id'].value_counts()#Takes count of number of session record each user have\n",
        "    present_ids=[]#Take ids of those whose session record is present\n",
        "    null_ids=[]#Takes ids of those whose session record is not there.\n",
        "    for i in range(len(user_counts)):\n",
        "        present_ids.append(user_counts.keys()[i])\n",
        "    left_out_ids=list(set(user_data['id'])-set(sessions_data['user_id']))#Ids that are in users data but not even one record in sessions data\n",
        "    null_ids.extend(left_out_ids)\n",
        "    \n",
        "    #present_sessions variable preprocesses sessions dataset for those ids whose session record is present\n",
        "    if len(present_ids)>=1:\n",
        "      present_sessions=sessions_data.loc[sessions_data['user_id'].isin(present_ids)]    \n",
        "      preprocessed_present_sessions=preprocess_sessions(present_sessions)\n",
        "      preprocessed_sessions=preprocessed_present_sessions\n",
        "    \n",
        "    #For those ids whose sessions dataset is not given are given zero values for all the features that are extracted from sessions dataset\n",
        "    if len(null_ids)>=1:\n",
        "      preprocessed_null_sessions=[]\n",
        "      for i in range(len(null_ids)):\n",
        "          null_session=[0]*667\n",
        "          null_session=[null_ids[i]]+null_session\n",
        "          preprocessed_null_sessions.append(null_session)\n",
        "      preprocessed_null_sessions=np.array(preprocessed_null_sessions)\n",
        "      column_names=sessions_col_names()\n",
        "      preprocessed_null_sessions=pd.DataFrame(preprocessed_null_sessions,columns=column_names)\n",
        "      preprocessed_null_sessions.drop(['unkown_action_type (action type presence)%'],axis=1,inplace=True) \n",
        "      if len(present_ids)>=1:\n",
        "        preprocessed_sessions=pd.concat([preprocessed_sessions,preprocessed_null_sessions],axis=0)\n",
        "      else:\n",
        "        preprocessed_sessions=preprocessed_null_sessions\n",
        "\n",
        "  #If no sessions data is given the following else code snippet executes\n",
        "  else:\n",
        "    null_ids=user_data['id'].tolist()\n",
        "    preprocessed_null_sessions=[]\n",
        "    for i in range(len(null_ids)):\n",
        "        null_session=[0]*667\n",
        "        null_session=[null_ids[i]]+null_session\n",
        "        preprocessed_null_sessions.append(null_session)\n",
        "    preprocessed_null_sessions=np.array(preprocessed_null_sessions)\n",
        "    column_names=sessions_col_names()\n",
        "    preprocessed_null_sessions=pd.DataFrame(preprocessed_null_sessions,columns=column_names)\n",
        "    preprocessed_null_sessions.drop(['unkown_action_type (action type presence)%'],axis=1,inplace=True) \n",
        "    preprocessed_sessions=preprocessed_null_sessions\n",
        "\n",
        "  final_df=preprocessed_sessions.merge(preprocessed_user,how='inner',left_on='id',right_on='id')\n",
        "  final_df=final_df.merge(user_to_target,how='inner',left_on='id',right_on='id')\n",
        "  target=final_df['target']\n",
        "  final_df.drop(['target'],axis=1,inplace=True)\n",
        "  final_df.drop(['id'],axis=1,inplace=True)\n",
        "  xgboost=XGBClassifier()\n",
        "  xgboost.load_model('xgboost_model.json')\n",
        "  le=load('class_label_encoder.bin') \n",
        "  target_value=le.transform(target)\n",
        "  proba_prediction=xgboost.predict_proba(np.array(final_df))\n",
        "  ndcg=ndcg_score(target_value,proba_prediction,k=5)\n",
        "  return ndcg\n",
        "  "
      ]
    }
  ]
}